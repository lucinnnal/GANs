{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4HYDyWuNxlsKs1CgMhPgP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucinnnal/GANs/blob/main/Generative_Adversarial_Networks_with_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HooypVtLdXo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image # 텐서를 이미지로 변환하고 지정된 경로에 저장할 수 있도록 도와줍니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "torchvision.utils.save_image(\n",
        "    tensor,             # 저장할 텐서\n",
        "    filename,           # 저장될 파일 경로 (파일명 포함)\n",
        "    nrow=1,             # 한 행에 표시할 이미지 수 (배열 형식 저장 시)\n",
        "    padding=2,          # 이미지 간의 패딩 (픽셀 단위)\n",
        "    normalize=False,    # 정규화 여부 (True로 설정 시, [0, 1] 범위로 스케일링)\n",
        "    range=None,         # 정규화 범위 (normalize=True일 때 유효)\n",
        "    scale_each=False,   # 각 이미지별로 정규화 여부\n",
        "    pad_value=0         # 패딩 영역의 값 (기본값: 0)\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "v4eh0MizMk__",
        "outputId": "89e3ff0b-f103-460d-deec-56383344dbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntorchvision.utils.save_image(\\n    tensor,             # 저장할 텐서\\n    filename,           # 저장될 파일 경로 (파일명 포함)\\n    nrow=1,             # 한 행에 표시할 이미지 수 (배열 형식 저장 시)\\n    padding=2,          # 이미지 간의 패딩 (픽셀 단위)\\n    normalize=False,    # 정규화 여부 (True로 설정 시, [0, 1] 범위로 스케일링)\\n    range=None,         # 정규화 범위 (normalize=True일 때 유효)\\n    scale_each=False,   # 각 이미지별로 정규화 여부\\n    pad_value=0         # 패딩 영역의 값 (기본값: 0)\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "9l81CauxYU_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator\n",
        "latent_dim = 100 # (noise vector dimension)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # the blocks that are appended to the layers (MLP)\n",
        "    def block(input_dim, output_dim, normalize = True):\n",
        "      layers = [nn.Linear(input_dim, output_dim)]\n",
        "      if normalize:\n",
        "        layers.append(nn.BatchNorm1d(output_dim, 0.8))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace = True)) # inplace = True 는 tensor 그 자체를 변환 시켜준다.\n",
        "      return layers\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        *block(latent_dim, 128, normalize = False),\n",
        "        *block(128, 256),\n",
        "        *block(256, 512),\n",
        "        *block(512,1024),\n",
        "        nn.Linear(1024, 1 * 28 * 28), # final image tensor creation\n",
        "        nn.Tanh() # range from -1 to 1 shape = [batch_size, 1 * 28 * 28]\n",
        "    )\n",
        "\n",
        "  def forward(self, z):\n",
        "    image = self.model(z)\n",
        "    image = image.view(image.size(0), 1, 28, 28) # image creation (flatten -> 3d tensor)\n",
        "    return image"
      ],
      "metadata": {
        "id": "lAlPQxH-OWWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(1*28*28, 512),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        nn.Linear(256,1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, img):\n",
        "    flatten = img.view(img.size(0), -1)\n",
        "    result = self.model(flatten)\n",
        "    return result"
      ],
      "metadata": {
        "id": "MEXQaPx5RyeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms\n",
        "transforms = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./dataset\", train=True, download=True, transform=transforms)\n",
        "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers = 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYUvChGwV5Ix",
        "outputId": "0eef073e-a72c-4b32-8234-3e627a622b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:11<00:00, 897kB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 243kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.39MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "generator.cuda()\n",
        "discriminator.to(device)\n",
        "\n",
        "adversarial_loss = nn.BCELoss()\n",
        "adversarial_loss.to(device)\n",
        "\n",
        "\n",
        "lr = 0.0002\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr = lr, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr = lr, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "ojK3aOgkV4xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer는 모델의 파라미터에 의존\n",
        "optimizer는 모델의 파라미터를 참조하며, 이 파라미터들이 이미 올바른 디바이스에 위치하고 있기 때문에, optimizer 자체를 별도로 다른 디바이스로 이동시킬 필요가 없습니다."
      ],
      "metadata": {
        "id": "87QYTeMIZ41X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "epochs = 200\n",
        "sample_interval = 2000\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "    real = torch.FloatTensor(imgs.size(0), 1).fill_(1.0).to(device) # 진짜(real): 1\n",
        "    fake = torch.FloatTensor(imgs.size(0), 1).fill_(0.0).to(device) # 가짜(fake): 0\n",
        "\n",
        "    real_imgs = imgs.to(device)\n",
        "\n",
        "    # Generator Training First\n",
        "    optimizer_G.zero_grad()\n",
        "    # noise sampling -> shape [batchsize, latent_dim]\n",
        "    noise = torch.normal(mean = 0, std = 1, size = (imgs.shape[0], latent_dim)).to(device)\n",
        "\n",
        "    # generate images\n",
        "    generated_imgs = generator(noise)\n",
        "\n",
        "    # generator loss calculation\n",
        "    generator_loss = adversarial_loss(discriminator(generated_imgs), real) # the generator ones should be trained to create the images that's similar to real ones -> purpose : discriminator will assume it as 1(real).\n",
        "\n",
        "    # backward & optimizer.step\n",
        "    generator_loss.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    # Discriminator Training Second -> The discriminator should real -> 1, fakes -> 0\n",
        "    optimizer_D.zero_grad()\n",
        "\n",
        "    real_loss = adversarial_loss(discriminator(real_imgs), real)\n",
        "    fake_loss = adversarial_loss(discriminator(generated_imgs.detach()), fake)\n",
        "\n",
        "    D_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "    D_loss.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    done = epoch * len(dataloader) + i # -> 현재까지 진행된 배치 수 출력!\n",
        "    if done % sample_interval == 0:\n",
        "        # 생성된 이미지 중에서 25개만 선택하여 5 X 5 격자 이미지에 출력\n",
        "        save_image(generated_imgs.data[:25], f\"{done}.png\", nrow=5, normalize=True)\n",
        "\n",
        "  # 하나의 epoch이 끝날 때마다 로그(log) 출력\n",
        "  print(f\"[Epoch {epoch}/{epochs}] [D loss: {D_loss.item():.6f}] [G loss: {generator_loss.item():.6f}] [Elapsed time: {time.time() - start_time:.2f}s]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMFUo4DDYq7U",
        "outputId": "4dcf3ec7-116f-422d-ef88-38ed376388d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/200] [D loss: 0.502041] [G loss: 0.966228] [Elapsed time: 16.93s]\n",
            "[Epoch 1/200] [D loss: 0.586556] [G loss: 0.424154] [Elapsed time: 32.59s]\n",
            "[Epoch 2/200] [D loss: 0.440869] [G loss: 0.715792] [Elapsed time: 48.88s]\n",
            "[Epoch 3/200] [D loss: 0.332163] [G loss: 1.488542] [Elapsed time: 66.90s]\n",
            "[Epoch 4/200] [D loss: 0.268628] [G loss: 1.496626] [Elapsed time: 84.20s]\n",
            "[Epoch 5/200] [D loss: 0.210796] [G loss: 2.024996] [Elapsed time: 99.88s]\n",
            "[Epoch 6/200] [D loss: 0.268943] [G loss: 1.162457] [Elapsed time: 116.05s]\n",
            "[Epoch 7/200] [D loss: 0.532374] [G loss: 0.493675] [Elapsed time: 132.89s]\n",
            "[Epoch 8/200] [D loss: 0.289610] [G loss: 2.647757] [Elapsed time: 149.83s]\n",
            "[Epoch 9/200] [D loss: 0.294159] [G loss: 1.401614] [Elapsed time: 165.77s]\n",
            "[Epoch 10/200] [D loss: 0.229754] [G loss: 1.999683] [Elapsed time: 185.89s]\n",
            "[Epoch 11/200] [D loss: 0.299919] [G loss: 1.077437] [Elapsed time: 201.75s]\n",
            "[Epoch 12/200] [D loss: 0.209720] [G loss: 1.401726] [Elapsed time: 217.50s]\n",
            "[Epoch 13/200] [D loss: 0.197777] [G loss: 2.453346] [Elapsed time: 233.31s]\n",
            "[Epoch 14/200] [D loss: 0.168518] [G loss: 2.436136] [Elapsed time: 248.95s]\n",
            "[Epoch 15/200] [D loss: 0.132403] [G loss: 2.138170] [Elapsed time: 266.37s]\n",
            "[Epoch 16/200] [D loss: 0.218193] [G loss: 2.174667] [Elapsed time: 282.20s]\n",
            "[Epoch 17/200] [D loss: 0.258096] [G loss: 1.494189] [Elapsed time: 297.83s]\n",
            "[Epoch 18/200] [D loss: 0.228754] [G loss: 5.403651] [Elapsed time: 313.51s]\n",
            "[Epoch 19/200] [D loss: 0.252174] [G loss: 1.321857] [Elapsed time: 329.13s]\n",
            "[Epoch 20/200] [D loss: 0.315481] [G loss: 4.753245] [Elapsed time: 346.10s]\n",
            "[Epoch 21/200] [D loss: 0.178254] [G loss: 2.049257] [Elapsed time: 361.94s]\n",
            "[Epoch 22/200] [D loss: 0.473674] [G loss: 8.197226] [Elapsed time: 377.41s]\n",
            "[Epoch 23/200] [D loss: 0.221630] [G loss: 3.321555] [Elapsed time: 393.00s]\n",
            "[Epoch 24/200] [D loss: 0.158081] [G loss: 2.343548] [Elapsed time: 408.80s]\n",
            "[Epoch 25/200] [D loss: 0.158212] [G loss: 2.033032] [Elapsed time: 424.27s]\n",
            "[Epoch 26/200] [D loss: 0.117318] [G loss: 2.431704] [Elapsed time: 440.65s]\n",
            "[Epoch 27/200] [D loss: 0.115533] [G loss: 2.678653] [Elapsed time: 456.10s]\n",
            "[Epoch 28/200] [D loss: 0.172432] [G loss: 2.617510] [Elapsed time: 471.55s]\n",
            "[Epoch 29/200] [D loss: 0.080319] [G loss: 2.978610] [Elapsed time: 487.08s]\n",
            "[Epoch 30/200] [D loss: 0.127073] [G loss: 2.312065] [Elapsed time: 502.65s]\n",
            "[Epoch 31/200] [D loss: 0.173261] [G loss: 1.808122] [Elapsed time: 518.94s]\n",
            "[Epoch 32/200] [D loss: 0.168655] [G loss: 4.413032] [Elapsed time: 535.24s]\n",
            "[Epoch 33/200] [D loss: 0.109175] [G loss: 2.315769] [Elapsed time: 550.75s]\n",
            "[Epoch 34/200] [D loss: 0.126807] [G loss: 2.564908] [Elapsed time: 566.30s]\n",
            "[Epoch 35/200] [D loss: 0.318440] [G loss: 4.384910] [Elapsed time: 581.76s]\n",
            "[Epoch 36/200] [D loss: 0.185742] [G loss: 3.096779] [Elapsed time: 597.44s]\n",
            "[Epoch 37/200] [D loss: 0.467531] [G loss: 0.893827] [Elapsed time: 613.86s]\n",
            "[Epoch 38/200] [D loss: 0.176933] [G loss: 2.720858] [Elapsed time: 629.20s]\n",
            "[Epoch 39/200] [D loss: 0.288963] [G loss: 3.530346] [Elapsed time: 644.53s]\n",
            "[Epoch 40/200] [D loss: 0.199462] [G loss: 3.528930] [Elapsed time: 660.04s]\n",
            "[Epoch 41/200] [D loss: 0.220795] [G loss: 1.479578] [Elapsed time: 675.40s]\n",
            "[Epoch 42/200] [D loss: 0.184510] [G loss: 1.753086] [Elapsed time: 691.44s]\n",
            "[Epoch 43/200] [D loss: 1.392074] [G loss: 12.083774] [Elapsed time: 707.62s]\n",
            "[Epoch 44/200] [D loss: 0.140704] [G loss: 3.970210] [Elapsed time: 723.24s]\n",
            "[Epoch 45/200] [D loss: 0.170405] [G loss: 3.253979] [Elapsed time: 738.67s]\n",
            "[Epoch 46/200] [D loss: 0.883737] [G loss: 6.728942] [Elapsed time: 754.31s]\n",
            "[Epoch 47/200] [D loss: 0.143779] [G loss: 2.106323] [Elapsed time: 770.03s]\n",
            "[Epoch 48/200] [D loss: 0.111192] [G loss: 2.375552] [Elapsed time: 786.96s]\n",
            "[Epoch 49/200] [D loss: 0.166384] [G loss: 4.424240] [Elapsed time: 802.63s]\n",
            "[Epoch 50/200] [D loss: 0.265095] [G loss: 4.899804] [Elapsed time: 818.28s]\n",
            "[Epoch 51/200] [D loss: 0.088524] [G loss: 2.527626] [Elapsed time: 834.49s]\n",
            "[Epoch 52/200] [D loss: 0.150659] [G loss: 2.334821] [Elapsed time: 851.19s]\n",
            "[Epoch 53/200] [D loss: 0.435308] [G loss: 5.720181] [Elapsed time: 868.25s]\n",
            "[Epoch 54/200] [D loss: 0.100127] [G loss: 2.359073] [Elapsed time: 884.16s]\n",
            "[Epoch 55/200] [D loss: 0.125238] [G loss: 2.455587] [Elapsed time: 899.97s]\n",
            "[Epoch 56/200] [D loss: 0.528476] [G loss: 8.196687] [Elapsed time: 915.41s]\n",
            "[Epoch 57/200] [D loss: 0.254475] [G loss: 3.085630] [Elapsed time: 931.12s]\n",
            "[Epoch 58/200] [D loss: 0.249260] [G loss: 5.423586] [Elapsed time: 947.37s]\n",
            "[Epoch 59/200] [D loss: 0.230135] [G loss: 3.564048] [Elapsed time: 963.12s]\n",
            "[Epoch 60/200] [D loss: 0.164721] [G loss: 2.816687] [Elapsed time: 978.58s]\n",
            "[Epoch 61/200] [D loss: 0.353982] [G loss: 5.508916] [Elapsed time: 994.41s]\n",
            "[Epoch 62/200] [D loss: 0.159715] [G loss: 2.547193] [Elapsed time: 1010.20s]\n",
            "[Epoch 63/200] [D loss: 0.095129] [G loss: 5.601893] [Elapsed time: 1027.26s]\n",
            "[Epoch 64/200] [D loss: 0.182571] [G loss: 1.585025] [Elapsed time: 1043.08s]\n",
            "[Epoch 65/200] [D loss: 0.146476] [G loss: 2.051314] [Elapsed time: 1059.10s]\n",
            "[Epoch 66/200] [D loss: 0.149653] [G loss: 4.929016] [Elapsed time: 1074.79s]\n",
            "[Epoch 67/200] [D loss: 0.234700] [G loss: 3.516676] [Elapsed time: 1090.75s]\n",
            "[Epoch 68/200] [D loss: 0.149360] [G loss: 2.283187] [Elapsed time: 1107.75s]\n",
            "[Epoch 69/200] [D loss: 0.287768] [G loss: 6.925430] [Elapsed time: 1123.96s]\n",
            "[Epoch 70/200] [D loss: 0.166601] [G loss: 3.014236] [Elapsed time: 1140.04s]\n",
            "[Epoch 71/200] [D loss: 0.193971] [G loss: 2.990904] [Elapsed time: 1156.91s]\n",
            "[Epoch 72/200] [D loss: 0.082849] [G loss: 4.588702] [Elapsed time: 1174.40s]\n",
            "[Epoch 73/200] [D loss: 0.187125] [G loss: 5.344089] [Elapsed time: 1191.86s]\n",
            "[Epoch 74/200] [D loss: 0.391283] [G loss: 6.874530] [Elapsed time: 1208.97s]\n",
            "[Epoch 75/200] [D loss: 0.178015] [G loss: 1.713310] [Elapsed time: 1225.59s]\n",
            "[Epoch 76/200] [D loss: 0.141903] [G loss: 2.265315] [Elapsed time: 1243.33s]\n",
            "[Epoch 77/200] [D loss: 0.056215] [G loss: 3.406033] [Elapsed time: 1259.28s]\n",
            "[Epoch 78/200] [D loss: 3.613338] [G loss: 21.279772] [Elapsed time: 1275.84s]\n",
            "[Epoch 79/200] [D loss: 0.198986] [G loss: 4.406693] [Elapsed time: 1292.20s]\n",
            "[Epoch 80/200] [D loss: 0.164820] [G loss: 5.661205] [Elapsed time: 1309.09s]\n",
            "[Epoch 81/200] [D loss: 0.157909] [G loss: 1.756749] [Elapsed time: 1326.38s]\n",
            "[Epoch 82/200] [D loss: 0.131531] [G loss: 2.324930] [Elapsed time: 1342.53s]\n",
            "[Epoch 83/200] [D loss: 0.141474] [G loss: 2.362041] [Elapsed time: 1358.67s]\n",
            "[Epoch 84/200] [D loss: 0.183542] [G loss: 2.594806] [Elapsed time: 1374.72s]\n",
            "[Epoch 85/200] [D loss: 0.088765] [G loss: 2.946573] [Elapsed time: 1392.16s]\n",
            "[Epoch 86/200] [D loss: 0.084119] [G loss: 2.989449] [Elapsed time: 1408.53s]\n",
            "[Epoch 87/200] [D loss: 0.091112] [G loss: 4.233334] [Elapsed time: 1424.74s]\n",
            "[Epoch 88/200] [D loss: 0.116890] [G loss: 3.532198] [Elapsed time: 1440.59s]\n",
            "[Epoch 89/200] [D loss: 0.142820] [G loss: 2.773872] [Elapsed time: 1457.47s]\n",
            "[Epoch 90/200] [D loss: 0.138697] [G loss: 3.168108] [Elapsed time: 1474.69s]\n",
            "[Epoch 91/200] [D loss: 0.320403] [G loss: 6.050007] [Elapsed time: 1490.95s]\n",
            "[Epoch 92/200] [D loss: 0.206272] [G loss: 2.745405] [Elapsed time: 1507.00s]\n",
            "[Epoch 93/200] [D loss: 0.158754] [G loss: 1.719886] [Elapsed time: 1523.25s]\n",
            "[Epoch 94/200] [D loss: 0.098313] [G loss: 2.804431] [Elapsed time: 1540.57s]\n",
            "[Epoch 95/200] [D loss: 0.103947] [G loss: 2.717755] [Elapsed time: 1556.81s]\n",
            "[Epoch 96/200] [D loss: 0.190796] [G loss: 2.511806] [Elapsed time: 1573.00s]\n",
            "[Epoch 97/200] [D loss: 0.110777] [G loss: 3.359202] [Elapsed time: 1588.80s]\n",
            "[Epoch 98/200] [D loss: 0.050638] [G loss: 3.983273] [Elapsed time: 1604.79s]\n",
            "[Epoch 99/200] [D loss: 0.247314] [G loss: 3.201845] [Elapsed time: 1621.94s]\n",
            "[Epoch 100/200] [D loss: 0.142793] [G loss: 2.352602] [Elapsed time: 1637.99s]\n",
            "[Epoch 101/200] [D loss: 0.174806] [G loss: 2.738348] [Elapsed time: 1653.84s]\n",
            "[Epoch 102/200] [D loss: 0.227913] [G loss: 3.048126] [Elapsed time: 1669.88s]\n",
            "[Epoch 103/200] [D loss: 0.070710] [G loss: 3.663057] [Elapsed time: 1686.31s]\n",
            "[Epoch 104/200] [D loss: 0.168355] [G loss: 2.860478] [Elapsed time: 1702.93s]\n",
            "[Epoch 105/200] [D loss: 0.102251] [G loss: 3.479673] [Elapsed time: 1719.02s]\n",
            "[Epoch 106/200] [D loss: 0.131574] [G loss: 2.863242] [Elapsed time: 1734.98s]\n",
            "[Epoch 107/200] [D loss: 0.284939] [G loss: 1.833346] [Elapsed time: 1751.00s]\n",
            "[Epoch 108/200] [D loss: 0.138089] [G loss: 2.382715] [Elapsed time: 1767.76s]\n",
            "[Epoch 109/200] [D loss: 0.258271] [G loss: 1.707583] [Elapsed time: 1784.43s]\n",
            "[Epoch 110/200] [D loss: 0.100415] [G loss: 2.523139] [Elapsed time: 1800.29s]\n",
            "[Epoch 111/200] [D loss: 0.093146] [G loss: 4.327157] [Elapsed time: 1816.26s]\n",
            "[Epoch 112/200] [D loss: 0.103541] [G loss: 2.504541] [Elapsed time: 1832.05s]\n",
            "[Epoch 113/200] [D loss: 0.047221] [G loss: 4.678210] [Elapsed time: 1848.39s]\n",
            "[Epoch 114/200] [D loss: 0.085280] [G loss: 3.529159] [Elapsed time: 1864.52s]\n",
            "[Epoch 115/200] [D loss: 0.062448] [G loss: 4.212381] [Elapsed time: 1879.95s]\n",
            "[Epoch 116/200] [D loss: 0.179535] [G loss: 4.045818] [Elapsed time: 1895.60s]\n",
            "[Epoch 117/200] [D loss: 0.163575] [G loss: 3.352836] [Elapsed time: 1911.96s]\n",
            "[Epoch 118/200] [D loss: 0.116845] [G loss: 2.742461] [Elapsed time: 1927.62s]\n",
            "[Epoch 119/200] [D loss: 0.125243] [G loss: 3.296808] [Elapsed time: 1944.60s]\n",
            "[Epoch 120/200] [D loss: 0.071150] [G loss: 3.472527] [Elapsed time: 1960.62s]\n",
            "[Epoch 121/200] [D loss: 0.117664] [G loss: 2.044265] [Elapsed time: 1976.91s]\n",
            "[Epoch 122/200] [D loss: 0.106172] [G loss: 3.524647] [Elapsed time: 1992.61s]\n",
            "[Epoch 123/200] [D loss: 0.076154] [G loss: 3.124954] [Elapsed time: 2008.37s]\n",
            "[Epoch 124/200] [D loss: 0.210666] [G loss: 1.801871] [Elapsed time: 2024.85s]\n",
            "[Epoch 125/200] [D loss: 0.164153] [G loss: 2.752127] [Elapsed time: 2040.38s]\n",
            "[Epoch 126/200] [D loss: 0.121242] [G loss: 2.691928] [Elapsed time: 2055.85s]\n",
            "[Epoch 127/200] [D loss: 0.137631] [G loss: 5.184991] [Elapsed time: 2072.42s]\n",
            "[Epoch 128/200] [D loss: 0.109410] [G loss: 3.260300] [Elapsed time: 2088.66s]\n",
            "[Epoch 129/200] [D loss: 0.074801] [G loss: 3.877127] [Elapsed time: 2105.84s]\n",
            "[Epoch 130/200] [D loss: 0.253048] [G loss: 7.478820] [Elapsed time: 2122.30s]\n",
            "[Epoch 131/200] [D loss: 0.099903] [G loss: 3.816439] [Elapsed time: 2138.99s]\n",
            "[Epoch 132/200] [D loss: 0.146675] [G loss: 9.586653] [Elapsed time: 2156.14s]\n",
            "[Epoch 133/200] [D loss: 0.159520] [G loss: 5.550645] [Elapsed time: 2173.43s]\n",
            "[Epoch 134/200] [D loss: 0.150882] [G loss: 1.973197] [Elapsed time: 2189.83s]\n",
            "[Epoch 135/200] [D loss: 0.128310] [G loss: 3.614760] [Elapsed time: 2206.48s]\n",
            "[Epoch 136/200] [D loss: 0.123988] [G loss: 2.840657] [Elapsed time: 2223.00s]\n",
            "[Epoch 137/200] [D loss: 0.127739] [G loss: 2.469589] [Elapsed time: 2240.33s]\n",
            "[Epoch 138/200] [D loss: 0.092677] [G loss: 2.926564] [Elapsed time: 2257.29s]\n",
            "[Epoch 139/200] [D loss: 0.322451] [G loss: 6.147961] [Elapsed time: 2274.22s]\n",
            "[Epoch 140/200] [D loss: 0.065784] [G loss: 4.265485] [Elapsed time: 2291.27s]\n",
            "[Epoch 141/200] [D loss: 0.176893] [G loss: 2.053709] [Elapsed time: 2309.22s]\n",
            "[Epoch 142/200] [D loss: 0.054947] [G loss: 3.219251] [Elapsed time: 2325.99s]\n",
            "[Epoch 143/200] [D loss: 0.174072] [G loss: 2.881621] [Elapsed time: 2342.97s]\n",
            "[Epoch 144/200] [D loss: 0.055137] [G loss: 4.049480] [Elapsed time: 2360.10s]\n",
            "[Epoch 145/200] [D loss: 0.082784] [G loss: 2.370457] [Elapsed time: 2377.85s]\n",
            "[Epoch 146/200] [D loss: 0.064609] [G loss: 4.422352] [Elapsed time: 2394.77s]\n",
            "[Epoch 147/200] [D loss: 0.217385] [G loss: 3.782117] [Elapsed time: 2411.72s]\n",
            "[Epoch 148/200] [D loss: 0.134147] [G loss: 3.379225] [Elapsed time: 2429.67s]\n",
            "[Epoch 149/200] [D loss: 0.102853] [G loss: 4.299016] [Elapsed time: 2446.50s]\n",
            "[Epoch 150/200] [D loss: 0.128728] [G loss: 2.367287] [Elapsed time: 2463.38s]\n",
            "[Epoch 151/200] [D loss: 0.332991] [G loss: 6.602271] [Elapsed time: 2480.06s]\n",
            "[Epoch 152/200] [D loss: 0.122981] [G loss: 2.763132] [Elapsed time: 2498.08s]\n",
            "[Epoch 153/200] [D loss: 0.048048] [G loss: 4.410477] [Elapsed time: 2514.69s]\n",
            "[Epoch 154/200] [D loss: 0.183315] [G loss: 3.753582] [Elapsed time: 2531.54s]\n",
            "[Epoch 155/200] [D loss: 0.206320] [G loss: 2.556568] [Elapsed time: 2548.33s]\n",
            "[Epoch 156/200] [D loss: 0.137516] [G loss: 2.834564] [Elapsed time: 2566.08s]\n",
            "[Epoch 157/200] [D loss: 0.661005] [G loss: 2.836227] [Elapsed time: 2582.73s]\n",
            "[Epoch 158/200] [D loss: 0.140946] [G loss: 4.078501] [Elapsed time: 2599.21s]\n",
            "[Epoch 159/200] [D loss: 0.107965] [G loss: 3.240720] [Elapsed time: 2615.72s]\n",
            "[Epoch 160/200] [D loss: 0.149040] [G loss: 4.320401] [Elapsed time: 2633.39s]\n",
            "[Epoch 161/200] [D loss: 0.173562] [G loss: 2.927883] [Elapsed time: 2650.00s]\n",
            "[Epoch 162/200] [D loss: 0.116439] [G loss: 3.701273] [Elapsed time: 2666.42s]\n",
            "[Epoch 163/200] [D loss: 0.105457] [G loss: 2.679552] [Elapsed time: 2683.07s]\n",
            "[Epoch 164/200] [D loss: 0.177641] [G loss: 3.004084] [Elapsed time: 2700.93s]\n",
            "[Epoch 165/200] [D loss: 1.015789] [G loss: 13.967975] [Elapsed time: 2717.31s]\n",
            "[Epoch 166/200] [D loss: 0.075853] [G loss: 3.035376] [Elapsed time: 2733.86s]\n",
            "[Epoch 167/200] [D loss: 0.241018] [G loss: 5.916575] [Elapsed time: 2750.40s]\n",
            "[Epoch 168/200] [D loss: 0.066011] [G loss: 3.453668] [Elapsed time: 2768.78s]\n",
            "[Epoch 169/200] [D loss: 0.243171] [G loss: 5.453406] [Elapsed time: 2786.02s]\n",
            "[Epoch 170/200] [D loss: 0.177868] [G loss: 3.005400] [Elapsed time: 2802.70s]\n",
            "[Epoch 171/200] [D loss: 0.117856] [G loss: 2.684102] [Elapsed time: 2819.29s]\n",
            "[Epoch 172/200] [D loss: 0.076451] [G loss: 2.712086] [Elapsed time: 2837.19s]\n",
            "[Epoch 173/200] [D loss: 0.093638] [G loss: 2.505484] [Elapsed time: 2853.93s]\n",
            "[Epoch 174/200] [D loss: 0.075727] [G loss: 3.476090] [Elapsed time: 2870.41s]\n",
            "[Epoch 175/200] [D loss: 0.404551] [G loss: 5.234658] [Elapsed time: 2887.25s]\n",
            "[Epoch 176/200] [D loss: 0.179546] [G loss: 5.333791] [Elapsed time: 2905.37s]\n",
            "[Epoch 177/200] [D loss: 0.246786] [G loss: 1.885919] [Elapsed time: 2922.11s]\n",
            "[Epoch 178/200] [D loss: 0.155073] [G loss: 3.344259] [Elapsed time: 2938.67s]\n",
            "[Epoch 179/200] [D loss: 0.125048] [G loss: 3.192360] [Elapsed time: 2955.19s]\n",
            "[Epoch 180/200] [D loss: 0.228774] [G loss: 5.758057] [Elapsed time: 2973.03s]\n",
            "[Epoch 181/200] [D loss: 0.178182] [G loss: 2.886696] [Elapsed time: 2989.96s]\n",
            "[Epoch 182/200] [D loss: 0.146781] [G loss: 3.034980] [Elapsed time: 3006.53s]\n",
            "[Epoch 183/200] [D loss: 0.155709] [G loss: 3.907658] [Elapsed time: 3023.03s]\n",
            "[Epoch 184/200] [D loss: 0.152069] [G loss: 2.410627] [Elapsed time: 3040.76s]\n",
            "[Epoch 185/200] [D loss: 0.097378] [G loss: 3.276157] [Elapsed time: 3057.41s]\n",
            "[Epoch 186/200] [D loss: 0.064536] [G loss: 2.928489] [Elapsed time: 3074.22s]\n",
            "[Epoch 187/200] [D loss: 0.161951] [G loss: 4.444524] [Elapsed time: 3090.56s]\n",
            "[Epoch 188/200] [D loss: 0.187969] [G loss: 5.404696] [Elapsed time: 3108.32s]\n",
            "[Epoch 189/200] [D loss: 0.193664] [G loss: 4.507020] [Elapsed time: 3124.68s]\n",
            "[Epoch 190/200] [D loss: 0.204323] [G loss: 3.433408] [Elapsed time: 3141.15s]\n",
            "[Epoch 191/200] [D loss: 0.113858] [G loss: 3.347959] [Elapsed time: 3157.88s]\n",
            "[Epoch 192/200] [D loss: 0.135564] [G loss: 6.043833] [Elapsed time: 3175.75s]\n",
            "[Epoch 193/200] [D loss: 0.135904] [G loss: 3.263194] [Elapsed time: 3192.42s]\n",
            "[Epoch 194/200] [D loss: 0.112027] [G loss: 3.769989] [Elapsed time: 3208.88s]\n",
            "[Epoch 195/200] [D loss: 0.138579] [G loss: 2.346138] [Elapsed time: 3225.51s]\n",
            "[Epoch 196/200] [D loss: 0.197529] [G loss: 1.745444] [Elapsed time: 3243.07s]\n",
            "[Epoch 197/200] [D loss: 0.105121] [G loss: 2.995074] [Elapsed time: 3259.78s]\n",
            "[Epoch 198/200] [D loss: 0.108083] [G loss: 2.109189] [Elapsed time: 3276.15s]\n",
            "[Epoch 199/200] [D loss: 0.132787] [G loss: 3.419281] [Elapsed time: 3292.69s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**왜 generated_imgs.detach()를 사용하는가?**\n",
        "GAN의 학습 과정에서, 생성자와 판별자는 동시에 학습됩니다. 하지만 각자의 손실 함수는 다른 역할을 합니다.\n",
        "**discriminator**는 진짜 이미지와 가짜 이미지를 구별하는 학습을 합니다.\n",
        "**generator**는 discriminator를 속이려고 가짜 이미지를 생성하는 학습을 합니다.\n",
        "**discriminator는 가짜 이미지에 대한 손실을 계산할 때, generated_imgs.detach()**를 사용하여 generator의 기울기가 계산되지 않도록 합니다. 이렇게 하면 discriminator가 가짜 이미지에 대한 손실을 계산하는 데만 집중하게 되며, generator의 파라미터는 영향을 받지 않게 됩니다.\n",
        "generator의 학습은 오직 generator가 생성한 이미지에 대한 손실만을 반영해야 하기 때문에, detached된 generated_imgs는 discriminator에서만 사용되고, generator의 기울기는 업데이트되지 않습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "-> discriminator를 학습할 때 generated_imgs 가 generator의 파라미터를 업데이트 하는데 영향을 미치지 않도록 하기 위해 사용됨. (generated_imgs 는 생성자에 의해 만들어진 텐서임을 고려)"
      ],
      "metadata": {
        "id": "Otr1iEtZupc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# torch.cuda.FloatTensor와 to(device)를 혼용할 경우, **텐서의 장치(device)**와 데이터 타입이 일치하지 않으면 연산 중 문제가 발생할 수 있습니다. 특히, 텐서를 서로 다른 장치로 이동시키거나 텐서의 데이터 타입을 일관성 있게 맞추지 않으면 불일치로 인한 오류가 발생할 수 있습니다."
      ],
      "metadata": {
        "id": "AJQBqfWs14FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image test\n",
        "from IPython.display import Image\n",
        "\n",
        "Image('92000.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "AgTV5T0t17q_",
        "outputId": "03be4e79-71f1-4eed-f594-27db3844dd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}